{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TR_Capt_Generation_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kUqrjgt2_Ifa","colab_type":"text"},"source":["# Model Training\n","\n","### Documents structure:\n","1.   Evaluation system settings\n","2.   Utility functions\n","3.   Input Data pre-processing\n","4.   Dictionaries generation\n","5.   Define Encoder-Decoder LSTM Architecture\n","6.   Evaluation metrics and functions\n","7.   Saving the trained models, Encoder and Decoder.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TOEKFKIrg1RN","colab_type":"code","outputId":"565a1b4c-70cb-4ab6-bcfd-708254b93876","executionInfo":{"status":"ok","timestamp":1580287118964,"user_tz":-60,"elapsed":30295,"user":{"displayName":"Andrea Spreafico","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mATj03igkSO2DcU6EeDRH0wQOCXWWeaPr8UEmWdWA=s64","userId":"05084839728425952645"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"source":["import re\n","import json\n","import string\n","import numpy as np\n","import pandas as pd\n","from string import digits\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from keras.layers import Input, LSTM, Embedding, Dense\n","from keras.models import Model\n","from keras.utils import plot_model\n","from keras import optimizers\n","\n","drive.mount('/content/drive')\n","\n","##########################\n","# CONFIGURATION SETTINGS #\n","config_number_str = \"v7\"\n","home_dir = \"/content/drive/My Drive/Current Works/UBC Research Period/Training Folder/\"\n","\n","# Encoder-Decoder LSTM Architecture Configuration\n","units_number = 512\n","encoder_dropout = 0.2\n","decoder_dropout = 0.2\n","embedding_size = 128\n","learning_rate = 0.001\n","epochs_number = 300\n","batch_size = 32\n","validation_split = 0.2\n","\n","##########################\n","\n","# Files loading\n","file_dir = home_dir + \"Datasets/5_Fold_Cross_Validation_time_series/5_train_time_series.txt\"\n","lines= pd.read_table(file_dir, sep=\"___\", names=['geo', 'year', 'title', 'uom', 'min_value', 'max_value', 'inp', 'out'])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lv8JeNY8hTIq","colab_type":"code","colab":{}},"source":["#####################\n","# UTILITY FUNCTIONS #\n","#####################\n","\n","def sentences_pre_processing(lines):\n","    \n","    # Defining the chars to be replaced and the relative substitute value.\n","    replacing_dictionary = {\"'\"  : '',             \n","                            \"\\\"\" : '',\n","                            ','  : ' COMMA ', \n","                            ':'  : ' COLON ', \n","                            \";\"  : ' SEMICOLON ',\n","                            '('  : ' S_R_BRACKET ', \n","                            ')'  : ' E_R_BRACKET ',\n","                            '.'  : ' _SEQ_END SEQ_START_ ',\n","                           }\n","    \n","    # Replacing all the chars and tokens within the 'replacing_dictionary'\n","    for idx in replacing_dictionary:\n","      lines.out=lines.out.apply(lambda x: x.replace(idx, replacing_dictionary[idx]))\n","\n","    # Adding tokens at the start and at the end of the caption.\n","    lines.out = lines.out.apply(lambda x : 'CAP_START_ SEQ_START_ '+ x + ' _SEQ_END _CAP_END')\n","    lines.out=lines.out.apply(lambda x: x.replace(' _SEQ_END SEQ_START_ _SEQ_END _CAP_END', ' _SEQ_END _CAP_END'))\n","\n","    \n","    return lines\n","\n","# Save a dictionary as JSON Object\n","def save_dictionary(data, save_dir):\n","  with open(save_dir, 'w') as fp:\n","    json.dump(data, fp, sort_keys=True, indent=4)\n","  print(\"Dictionary correctly saved!\")\n","\n","# Load a dictionary as JSON Object\n","def load_dictionary(load_dir):\n","  with open(load_dir, 'r') as fp:\n","      data = json.load(fp)    \n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1-11rW79dVi","colab_type":"code","colab":{}},"source":["### ### ### ### ### ### ### ### ### ###\n","# PRE PROCESSING THE INPUT DATA       #\n","### ### ### ### ### ### ### ### ### ###\n","\n","lines = sentences_pre_processing(lines)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S50p01-c6bQj","colab_type":"code","outputId":"871ddfff-7de2-4fa1-b63a-c097212af041","executionInfo":{"status":"ok","timestamp":1580287121314,"user_tz":-60,"elapsed":32489,"user":{"displayName":"Andrea Spreafico","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mATj03igkSO2DcU6EeDRH0wQOCXWWeaPr8UEmWdWA=s64","userId":"05084839728425952645"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["### ### ### ### ### ### ### ### ### ####\n","# GENERATING THE TRAINING DICTIONARIES #\n","### ### ### ### ### ### ### ### ### ####\n","\n","all_out_words=set()\n","for out in lines.out:\n","  for word in out.split():\n","    if word not in all_out_words:\n","      all_out_words.add(word)\n","target_words = sorted(list(all_out_words))\n","num_decoder_tokens = len(all_out_words) + 1\n","target_token_index = dict([(word, i) for i, word in enumerate(target_words, 1)])\n","\n","# Reverse-lookup token index to decode sequences back to something readable.\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","# Intiailizing the encoder/decoder arrays\n","decoder_input_data = np.zeros((len(lines.out), len(target_words)), dtype='float32')\n","decoder_target_data = np.zeros((len(lines.out),  len(target_words), num_decoder_tokens), dtype='float32')\n","encoder_input_data = np.zeros((len(lines.inp), 12), dtype='float32')\n","\n","for idx, row in enumerate(lines.inp):\n","  for month, value in enumerate(row.split(\" \")[:-1]):\n","    encoder_input_data[idx, month] = float(value)\n","\n","for i, (input_text, target_text) in enumerate(zip(lines.inp, lines.out)):\n","    for t, word in enumerate(target_text.split()):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t] = target_token_index[word]\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n","\n","print(reverse_target_char_index)\n","# Save dictionaries about training set\n","save_dictionary(target_token_index, home_dir + \"Dictionaries/\" + config_number_str + \"_target_token_index.json\")\n","save_dictionary(reverse_target_char_index, home_dir + \"Dictionaries/\"  + config_number_str + \"_reverse_target_char_index.json\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["{1: '$', 2: '-1', 3: '-11', 4: '-2', 5: '-3', 6: '-32', 7: '-4', 8: '-48', 9: '0', 10: '1', 11: '10', 12: '100', 13: '101', 14: '102', 15: '103', 16: '104', 17: '105', 18: '108', 19: '11', 20: '110', 21: '12', 22: '13', 23: '15', 24: '16', 25: '19', 26: '2', 27: '20', 28: '2016', 29: '2018', 30: '2019', 31: '21', 32: '24', 33: '25', 34: '27', 35: '28', 36: '3', 37: '30', 38: '33', 39: '34', 40: '35', 41: '36', 42: '37', 43: '38', 44: '4', 45: '40', 46: '41', 47: '42', 48: '43', 49: '44', 50: '45', 51: '47', 52: '48', 53: '50', 54: '52', 55: '53', 56: '56', 57: '59', 58: '6', 59: '60', 60: '61', 61: '62', 62: '63', 63: '64', 64: '65', 65: '66', 66: '69', 67: '7', 68: '70', 69: '71', 70: '73', 71: '74', 72: '75', 73: '77', 74: '79', 75: '80', 76: '81', 77: '85', 78: '88', 79: '89', 80: '9', 81: '90', 82: '91', 83: '92', 84: '93', 85: '94', 86: '95', 87: '96', 88: '97', 89: '98', 90: '99', 91: 'CAP_START_', 92: 'COLON', 93: 'COMMA', 94: 'E_R_BRACKET', 95: 'SEMICOLON', 96: 'SEQ_START_', 97: 'S_R_BRACKET', 98: 'TKN_About', 99: 'TKN_Geo', 100: 'TKN_UOM', 101: 'TKN_Year', 102: '_CAP_END', 103: '_SEQ_END', 104: 'a', 105: 'about', 106: 'above', 107: 'absolute', 108: 'accompanying', 109: 'according', 110: 'achieved', 111: 'acute', 112: 'aforementioned', 113: 'after', 114: 'afterwards', 115: 'again', 116: 'agin', 117: 'all', 118: 'all-year', 119: 'almost', 120: 'also', 121: 'ambiguous', 122: 'amount', 123: 'amout', 124: 'an', 125: 'analyzed', 126: 'and', 127: 'annual', 128: 'another', 129: 'anual', 130: 'any', 131: 'approximately', 132: 'april', 133: 'are', 134: 'around', 135: 'as', 136: 'ascendant', 137: 'assume', 138: 'at', 139: 'aug', 140: 'august', 141: 'august-november', 142: 'average', 143: 'averaged', 144: 'back', 145: 'back-to-school', 146: 'basically', 147: 'be', 148: 'because', 149: 'been', 150: 'before', 151: 'beginning', 152: 'being', 153: 'below', 154: 'beside', 155: 'besides', 156: 'best', 157: 'between', 158: 'betwewen', 159: 'big', 160: 'bikes', 161: 'bit', 162: 'boost', 163: 'bottom', 164: 'boxing', 165: 'broken', 166: 'but', 167: 'butter', 168: 'buy', 169: 'by', 170: 'can', 171: 'canadian', 172: 'canda', 173: 'candle', 174: 'canola', 175: 'cars', 176: 'change', 177: 'changes', 178: 'chart', 179: 'charts', 180: 'clear', 181: 'clearly', 182: 'climb', 183: 'climbed', 184: 'climbing', 185: 'coinciding', 186: 'collected', 187: 'comes', 188: 'coming', 189: 'comparably', 190: 'comparison', 191: 'computer', 192: 'computers', 193: 'consecutively', 194: 'considerable', 195: 'considerably', 196: 'considered', 197: 'consistent', 198: 'consistently', 199: 'constanously', 200: 'constant', 201: 'constantly', 202: 'content', 203: 'continous', 204: 'continously', 205: 'continued', 206: 'continues', 207: 'continuous', 208: 'continuously', 209: 'continuslie', 210: 'continusliy', 211: 'continusly', 212: 'cost', 213: 'country', 214: 'course', 215: 'crashes', 216: 'crashing', 217: 'creamery', 218: 'cubic', 219: 'data', 220: 'day', 221: 'dec', 222: 'december', 223: 'decline', 224: 'declined', 225: 'declines', 226: 'declining', 227: 'decrase', 228: 'decreas', 229: 'decrease', 230: 'decreased', 231: 'decreases', 232: 'decreasing', 233: 'decribing', 234: 'definetely', 235: 'depicted', 236: 'depicts', 237: 'descendant', 238: 'descending', 239: 'descent', 240: 'described', 241: 'describes', 242: 'describing', 243: 'despite', 244: 'developement', 245: 'development', 246: 'devided', 247: 'diagram', 248: 'diagramm', 249: 'did', 250: 'differ', 251: 'different', 252: 'dip', 253: 'dips', 254: 'displayed', 255: 'displaying', 256: 'displays', 257: 'distinctive', 258: 'divided', 259: 'do', 260: 'does', 261: 'doesnt', 262: 'dollar', 263: 'dollars', 264: 'down', 265: 'downs', 266: 'downward', 267: 'downwards', 268: 'dramatic', 269: 'drastically', 270: 'drop', 271: 'dropped', 272: 'dropping', 273: 'drops', 274: 'due', 275: 'during', 276: 'e', 277: 'each', 278: 'eggs', 279: 'elucidates', 280: 'end', 281: 'enterin', 282: 'entering', 283: 'equally', 284: 'especially', 285: 'even', 286: 'every', 287: 'except', 288: 'exception', 289: 'experienced', 290: 'experiences', 291: 'experiencing', 292: 'fairly', 293: 'fall', 294: 'falling', 295: 'falls', 296: 'farm', 297: 'farmers', 298: 'feb', 299: 'february', 300: 'fell', 301: 'few', 302: 'figure', 303: 'final', 304: 'finally', 305: 'first', 306: 'firstly', 307: 'five', 308: 'flactuating', 309: 'flour', 310: 'fluactuating', 311: 'fluactuations', 312: 'fluctations', 313: 'fluctuate', 314: 'fluctuated', 315: 'fluctuates', 316: 'fluctuating', 317: 'fluctuation', 318: 'fluctuations', 319: 'fo', 320: 'follow', 321: 'followed', 322: 'following', 323: 'follows', 324: 'folowing', 325: 'for', 326: 'form', 327: 'found', 328: 'four', 329: 'fppi', 330: 'fpppi', 331: 'frequently', 332: 'fresh', 333: 'from', 334: 'fruit', 335: 'further', 336: 'furthermore', 337: 'futhermore', 338: 'g', 339: 'gain', 340: 'general', 341: 'generally', 342: 'given', 343: 'giving', 344: 'global', 345: 'goes', 346: 'going', 347: 'got', 348: 'gradual', 349: 'gradually', 350: 'grain', 351: 'grapg', 352: 'graph', 353: 'graphic', 354: 'graphs', 355: 'greatly', 356: 'grew', 357: 'grow', 358: 'growed', 359: 'growing', 360: 'grown', 361: 'grows', 362: 'growth', 363: 'guess', 364: 'had', 365: 'half', 366: 'happend', 367: 'hardly', 368: 'hardwood', 369: 'harvest', 370: 'has', 371: 'have', 372: 'having', 373: 'he', 374: 'hear', 375: 'heavily', 376: 'held', 377: 'high', 378: 'higher', 379: 'highes', 380: 'highest', 381: 'highs', 382: 'hole', 383: 'homogeneous', 384: 'houses', 385: 'how', 386: 'huge', 387: 'hugely', 388: 'i', 389: 'identify', 390: 'if', 391: 'illustrated', 392: 'illustrates', 393: 'illustrating', 394: 'image', 395: 'import', 396: 'important', 397: 'imports', 398: 'in', 399: 'incline', 400: 'increaes', 401: 'increase', 402: 'increased', 403: 'increases', 404: 'increasing', 405: 'incredibly', 406: 'index', 407: 'indicates', 408: 'indicating', 409: 'information', 410: 'initially', 411: 'interesting', 412: 'into', 413: 'is', 414: 'ist', 415: 'it', 416: 'ithe', 417: 'its', 418: 'jan', 419: 'jannuary', 420: 'januar', 421: 'januarry', 422: 'january', 423: 'january-june', 424: 'jul', 425: 'july', 426: 'jump', 427: 'jumps', 428: 'june', 429: 'june-august', 430: 'just', 431: 'keep', 432: 'keeps', 433: 'kept', 434: 'kind', 435: 'land', 436: 'laptop', 437: 'laptops', 438: 'last', 439: 'least', 440: 'less', 441: 'level', 442: 'levels', 443: 'lies', 444: 'light', 445: 'like', 446: 'line', 447: 'linearly', 448: 'lines', 449: 'little', 450: 'local', 451: 'lot', 452: 'low', 453: 'lower', 454: 'lowest', 455: 'lows', 456: 'm-shaped', 457: 'm3', 458: 'main', 459: 'mainly', 460: 'maintained', 461: 'mainted', 462: 'malt', 463: 'manner', 464: 'many', 465: 'march', 466: 'marked', 467: 'markedly', 468: 'market', 469: 'marks', 470: 'massive', 471: 'massivly', 472: 'matched', 473: 'maxima', 474: 'maximum', 475: 'maxumum', 476: 'may', 477: 'may-september', 478: 'measure', 479: 'measured', 480: 'measurement', 481: 'measures', 482: 'median', 483: 'meters', 484: 'middle', 485: 'middling', 486: 'mild', 487: 'minimal', 488: 'minimu', 489: 'minimum', 490: 'minor', 491: 'missing', 492: 'moderate', 493: 'mon', 494: 'money', 495: 'montes', 496: 'month', 497: 'monthes', 498: 'monthly', 499: 'months', 500: 'more', 501: 'moreover', 502: 'most', 503: 'mostly', 504: 'motor', 505: 'mounths', 506: 'much', 507: 'near', 508: 'nearly', 509: 'new', 510: 'next', 511: 'no', 512: 'none', 513: 'not', 514: 'note', 515: 'noticeable', 516: 'nov', 517: 'novembe', 518: 'november', 519: 'novermber', 520: 'number', 521: 'numbers', 522: 'oat', 523: 'oats', 524: 'obeyed', 525: 'observe', 526: 'observed', 527: 'occurred', 528: 'oct', 529: 'october', 530: 'october-december', 531: 'octoeber', 532: 'of', 533: 'off', 534: 'oil', 535: 'on', 536: 'one', 537: 'only', 538: 'onto', 539: 'onwards', 540: 'or', 541: 'original', 542: 'oscilates', 543: 'oscillate', 544: 'oscillated', 545: 'oscillates', 546: 'oscillating', 547: 'oscillations', 548: 'other', 549: 'others', 550: 'otherwise', 551: 'out', 552: 'over', 553: 'overall', 554: 'parable', 555: 'particular', 556: 'pattern', 557: 'patterns', 558: 'peak', 559: 'peaking', 560: 'peaks', 561: 'people', 562: 'per', 563: 'period', 564: 'periods', 565: 'phisiologic', 566: 'place', 567: 'plateaued', 568: 'plateauing', 569: 'plateaus', 570: 'plot', 571: 'plummeted', 572: 'plummets', 573: 'plummited', 574: 'plunges', 575: 'point', 576: 'possible', 577: 'possibly', 578: 'poultry', 579: 'poults', 580: 'presented', 581: 'presents', 582: 'previous', 583: 'price', 584: 'prices', 585: 'pricing', 586: 'pridce', 587: 'probably', 588: 'proceeded', 589: 'produced', 590: 'product', 591: 'producted', 592: 'productioin', 593: 'production', 594: 'productions', 595: 'products', 596: 'produzed', 597: 'profit', 598: 'provided', 599: 'provides', 600: 'punctual', 601: 'q1', 602: 'q2', 603: 'q3', 604: 'q4', 605: 'quantified', 606: 'quarter', 607: 'quick', 608: 'quite', 609: 'raise', 610: 'raised', 611: 'raises', 612: 'raising', 613: 'range', 614: 'ranges', 615: 'ranging', 616: 'rapid', 617: 'rapidly', 618: 'rapitly', 619: 'rate', 620: 'rather', 621: 'reach', 622: 'reached', 623: 'reaches', 624: 'reaching', 625: 'really', 626: 'rebound', 627: 'recorded', 628: 'recovered', 629: 'recovers', 630: 'reduction', 631: 'reflect', 632: 'reflects', 633: 'registered', 634: 'relative', 635: 'relatively', 636: 'relevant', 637: 'remain', 638: 'remainder', 639: 'remained', 640: 'remaining', 641: 'remains', 642: 'reported', 643: 'reports', 644: 'represented', 645: 'represents', 646: 'respective', 647: 'respectively', 648: 'rest', 649: 'resulting', 650: 'retail', 651: 'returning', 652: 'returns', 653: 'rise', 654: 'risen', 655: 'rises', 656: 'rising', 657: 'rose', 658: 'roughly', 659: 'rye', 660: 's', 661: 'sales', 662: 'same', 663: 'saw', 664: 'say', 665: 'season', 666: 'seasonal', 667: 'second', 668: 'see', 669: 'seem', 670: 'seen', 671: 'sep', 672: 'september', 673: 'september-december', 674: 'september/october', 675: 'servers', 676: 'serves', 677: 'setpember', 678: 'settled', 679: 'several', 680: 'sharp', 681: 'sharply', 682: 'shell', 683: 'shoes', 684: 'short', 685: 'show', 686: 'showed', 687: 'showen', 688: 'showes', 689: 'showing', 690: 'shown', 691: 'shows', 692: 'shrinks', 693: 'shy', 694: 'significant', 695: 'significantly', 696: 'similar', 697: 'similiar', 698: 'sits', 699: 'slghtly', 700: 'slide', 701: 'slight', 702: 'slightly', 703: 'slip', 704: 'slipped', 705: 'slow', 706: 'slowly', 707: 'small', 708: 'smallest', 709: 'smooth', 710: 'so', 711: 'soft', 712: 'softwood', 713: 'some', 714: 'sorted', 715: 'spent', 716: 'spike', 717: 'split', 718: 'spring', 719: 'stable', 720: 'stand', 721: 'stands', 722: 'start', 723: 'started', 724: 'starting', 725: 'starts', 726: 'states', 727: 'stay', 728: 'stayed', 729: 'staying', 730: 'stays', 731: 'steadily', 732: 'steady', 733: 'steadyly', 734: 'steep', 735: 'steeply', 736: 'stock', 737: 'stocks', 738: 'striking', 739: 'strong', 740: 'strongly', 741: 'subsequently', 742: 'substan', 743: 'substantial', 744: 'substantially', 745: 'sudden', 746: 'suddenly', 747: 'summer', 748: 'supply', 749: 'surges', 750: 'surging', 751: 'tablet', 752: 'taken', 753: 'takes', 754: 'taking', 755: 'tending', 756: 'than', 757: 'that', 758: 'the', 759: 'their', 760: 'them', 761: 'then', 762: 'there', 763: 'thereafter', 764: 'they', 765: 'third', 766: 'this', 767: 'those', 768: 'though', 769: 'thre', 770: 'three', 771: 'through', 772: 'throughout', 773: 'til', 774: 'till', 775: 'time', 776: 'times', 777: 'to', 778: 'toatal', 779: 'tones', 780: 'tons', 781: 'too', 782: 'top', 783: 'total', 784: 'totals', 785: 'towards', 786: 'travel', 787: 'trend', 788: 'trended', 789: 'trends', 790: 'tripled', 791: 'trough', 792: 'twice', 793: 'two', 794: 'unaltered', 795: 'unchanged', 796: 'under', 797: 'unfortunately', 798: 'unit', 799: 'unitated', 800: 'united', 801: 'until', 802: 'uom', 803: 'up', 804: 'ups', 805: 'uptimes', 806: 'upward', 807: 'used', 808: 'using', 809: 'vacation', 810: 'value', 811: 'values', 812: 'variation', 813: 'varied', 814: 'varies', 815: 'vary', 816: 'vegetables', 817: 'vehicel', 818: 'vehicle', 819: 'vehicles', 820: 'very', 821: 'vs', 822: 'was', 823: 'waveform', 824: 'way', 825: 'we', 826: 'week', 827: 'were', 828: 'what', 829: 'when', 830: 'where', 831: 'whereas', 832: 'which', 833: 'while', 834: 'whole', 835: 'wide', 836: 'will', 837: 'winter', 838: 'with', 839: 'within', 840: 'without', 841: 'wood', 842: 'work', 843: 'x-axis', 844: 'y-axis', 845: 'year', 846: 'yearly', 847: 'you', 848: 'zig-zag'}\n","Dictionary correctly saved!\n","Dictionary correctly saved!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wao-eko-giFj","colab_type":"code","outputId":"62ac223d-cd6b-49d6-9f96-4a2d53ab59d8","executionInfo":{"status":"ok","timestamp":1580317006723,"user_tz":-60,"elapsed":19972604,"user":{"displayName":"Andrea Spreafico","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mATj03igkSO2DcU6EeDRH0wQOCXWWeaPr8UEmWdWA=s64","userId":"05084839728425952645"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["### ### ### ### ### ### ### ### #### \n","# ENCODER-DECODER MODEL DEFINITION # \n","### ### ### ### ### ### ### ### #### \n","\n","# In order to change the architecture configuration,\n","# Check out the configuration section at the beginning of this document.\n","\n","### ### ### ### ### ### ###\n","# ENCODER MODEL STRUCTURE #\n","### ### ### ### ### ### ###\n","encoder_inputs = Input(shape=(None,))\n","en_x=  Embedding(101, embedding_size)(encoder_inputs)\n","encoder = LSTM(units_number, dropout=encoder_dropout, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(en_x)\n","# Discard the `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","### ### ### ### ### ### ###\n","# DECODER MODEL STRUCTURE #\n","### ### ### ### ### ### ###\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None,))\n","dex = Embedding(num_decoder_tokens, embedding_size)\n","final_dex= dex(decoder_inputs)\n","decoder_lstm = LSTM(units_number, dropout=decoder_dropout,return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","### ### ### ### ### ### ### ### ### #\n","# ENCODER - DECODER MODEL STRUCTURE #\n","### ### ### ### ### ### ### ### ### #\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n","model.summary()\n","\n","\n","from keras.callbacks import EarlyStopping\n","earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=5, restore_best_weights=True)\n","\n","# Fit the model\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs_number,\n","          validation_split=validation_split)\n","          #callbacks=[earlyStop])\n","\n","# Create sampling model\n","decoder_state_input_h = Input(shape=(units_number,))\n","decoder_state_input_c = Input(shape=(units_number,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","final_dex2= dex(decoder_inputs)\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n","decoder_states2 = [state_h2, state_c2]\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, None, 128)    12928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, None, 128)    108672      input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 512), (None, 1312768     embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, None, 512),  1312768     embedding_2[0][0]                \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, None, 849)    435537      lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 3,182,673\n","Trainable params: 3,182,673\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 196 samples, validate on 49 samples\n","Epoch 1/300\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","196/196 [==============================] - 95s 486ms/step - loss: 0.4899 - val_loss: 0.4728\n","Epoch 2/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.4055 - val_loss: 0.3656\n","Epoch 3/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.3556 - val_loss: 0.3596\n","Epoch 4/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.3472 - val_loss: 0.3570\n","Epoch 5/300\n","196/196 [==============================] - 91s 463ms/step - loss: 0.3432 - val_loss: 0.3525\n","Epoch 6/300\n","196/196 [==============================] - 89s 456ms/step - loss: 0.3382 - val_loss: 0.3496\n","Epoch 7/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.3340 - val_loss: 0.3462\n","Epoch 8/300\n","196/196 [==============================] - 92s 472ms/step - loss: 0.3298 - val_loss: 0.3427\n","Epoch 9/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.3259 - val_loss: 0.3392\n","Epoch 10/300\n","196/196 [==============================] - 93s 473ms/step - loss: 0.3216 - val_loss: 0.3359\n","Epoch 11/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.3172 - val_loss: 0.3315\n","Epoch 12/300\n","196/196 [==============================] - 91s 464ms/step - loss: 0.3125 - val_loss: 0.3274\n","Epoch 13/300\n","196/196 [==============================] - 92s 471ms/step - loss: 0.3082 - val_loss: 0.3235\n","Epoch 14/300\n","196/196 [==============================] - 92s 471ms/step - loss: 0.3036 - val_loss: 0.3190\n","Epoch 15/300\n","196/196 [==============================] - 92s 470ms/step - loss: 0.2994 - val_loss: 0.3151\n","Epoch 16/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.2944 - val_loss: 0.3112\n","Epoch 17/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.2893 - val_loss: 0.3066\n","Epoch 18/300\n","196/196 [==============================] - 94s 480ms/step - loss: 0.2834 - val_loss: 0.3008\n","Epoch 19/300\n","196/196 [==============================] - 95s 486ms/step - loss: 0.2766 - val_loss: 0.2948\n","Epoch 20/300\n","196/196 [==============================] - 95s 487ms/step - loss: 0.2697 - val_loss: 0.2890\n","Epoch 21/300\n","196/196 [==============================] - 94s 480ms/step - loss: 0.2626 - val_loss: 0.3139\n","Epoch 22/300\n","196/196 [==============================] - 92s 470ms/step - loss: 0.2561 - val_loss: 0.3096\n","Epoch 23/300\n","196/196 [==============================] - 92s 472ms/step - loss: 0.2500 - val_loss: 0.3054\n","Epoch 24/300\n","196/196 [==============================] - 93s 472ms/step - loss: 0.2446 - val_loss: 0.2708\n","Epoch 25/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.2395 - val_loss: 0.2671\n","Epoch 26/300\n","196/196 [==============================] - 90s 459ms/step - loss: 0.2349 - val_loss: 0.2643\n","Epoch 27/300\n","196/196 [==============================] - 93s 477ms/step - loss: 0.2302 - val_loss: 0.2622\n","Epoch 28/300\n","196/196 [==============================] - 95s 484ms/step - loss: 0.2259 - val_loss: 0.2597\n","Epoch 29/300\n","196/196 [==============================] - 93s 477ms/step - loss: 0.2223 - val_loss: 0.2571\n","Epoch 30/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.2188 - val_loss: 0.2548\n","Epoch 31/300\n","196/196 [==============================] - 93s 472ms/step - loss: 0.2153 - val_loss: 0.2530\n","Epoch 32/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.2115 - val_loss: 0.2515\n","Epoch 33/300\n","196/196 [==============================] - 93s 472ms/step - loss: 0.2084 - val_loss: 0.2501\n","Epoch 34/300\n","196/196 [==============================] - 91s 465ms/step - loss: 0.2055 - val_loss: 0.2486\n","Epoch 35/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.2034 - val_loss: 0.2469\n","Epoch 36/300\n","196/196 [==============================] - 93s 472ms/step - loss: 0.2004 - val_loss: 0.2456\n","Epoch 37/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.1978 - val_loss: 0.2446\n","Epoch 38/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.1950 - val_loss: 0.2436\n","Epoch 39/300\n","196/196 [==============================] - 90s 461ms/step - loss: 0.1929 - val_loss: 0.2422\n","Epoch 40/300\n","196/196 [==============================] - 91s 464ms/step - loss: 0.1902 - val_loss: 0.2410\n","Epoch 41/300\n","196/196 [==============================] - 90s 458ms/step - loss: 0.1883 - val_loss: 0.2403\n","Epoch 42/300\n","196/196 [==============================] - 90s 457ms/step - loss: 0.1856 - val_loss: 0.2393\n","Epoch 43/300\n","196/196 [==============================] - 89s 452ms/step - loss: 0.1836 - val_loss: 0.2381\n","Epoch 44/300\n","196/196 [==============================] - 89s 456ms/step - loss: 0.1814 - val_loss: 0.2384\n","Epoch 45/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.1793 - val_loss: 0.2374\n","Epoch 46/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.1773 - val_loss: 0.2370\n","Epoch 47/300\n","196/196 [==============================] - 92s 467ms/step - loss: 0.1755 - val_loss: 0.2365\n","Epoch 48/300\n","196/196 [==============================] - 93s 474ms/step - loss: 0.1738 - val_loss: 0.2362\n","Epoch 49/300\n","196/196 [==============================] - 94s 482ms/step - loss: 0.1717 - val_loss: 0.2352\n","Epoch 50/300\n","196/196 [==============================] - 104s 531ms/step - loss: 0.1698 - val_loss: 0.2340\n","Epoch 51/300\n","196/196 [==============================] - 105s 533ms/step - loss: 0.1680 - val_loss: 0.2338\n","Epoch 52/300\n","196/196 [==============================] - 97s 495ms/step - loss: 0.1662 - val_loss: 0.2334\n","Epoch 53/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.1647 - val_loss: 0.2330\n","Epoch 54/300\n","196/196 [==============================] - 97s 495ms/step - loss: 0.1630 - val_loss: 0.2327\n","Epoch 55/300\n","196/196 [==============================] - 90s 460ms/step - loss: 0.1613 - val_loss: 0.2325\n","Epoch 56/300\n","196/196 [==============================] - 90s 459ms/step - loss: 0.1601 - val_loss: 0.2322\n","Epoch 57/300\n","196/196 [==============================] - 90s 458ms/step - loss: 0.1585 - val_loss: 0.2314\n","Epoch 58/300\n","196/196 [==============================] - 89s 453ms/step - loss: 0.1568 - val_loss: 0.2315\n","Epoch 59/300\n","196/196 [==============================] - 90s 457ms/step - loss: 0.1549 - val_loss: 0.2312\n","Epoch 60/300\n","196/196 [==============================] - 95s 482ms/step - loss: 0.1536 - val_loss: 0.2305\n","Epoch 61/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.1519 - val_loss: 0.2300\n","Epoch 62/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.1508 - val_loss: 0.2304\n","Epoch 63/300\n","196/196 [==============================] - 109s 557ms/step - loss: 0.1495 - val_loss: 0.2298\n","Epoch 64/300\n","196/196 [==============================] - 109s 558ms/step - loss: 0.1488 - val_loss: 0.2299\n","Epoch 65/300\n","196/196 [==============================] - 108s 550ms/step - loss: 0.1476 - val_loss: 0.2289\n","Epoch 66/300\n","196/196 [==============================] - 111s 566ms/step - loss: 0.1457 - val_loss: 0.2292\n","Epoch 67/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.1444 - val_loss: 0.2285\n","Epoch 68/300\n","196/196 [==============================] - 92s 467ms/step - loss: 0.1430 - val_loss: 0.2292\n","Epoch 69/300\n","196/196 [==============================] - 90s 461ms/step - loss: 0.1414 - val_loss: 0.2289\n","Epoch 70/300\n","196/196 [==============================] - 90s 460ms/step - loss: 0.1401 - val_loss: 0.2285\n","Epoch 71/300\n","196/196 [==============================] - 89s 454ms/step - loss: 0.1392 - val_loss: 0.2285\n","Epoch 72/300\n","196/196 [==============================] - 89s 455ms/step - loss: 0.1380 - val_loss: 0.2283\n","Epoch 73/300\n","196/196 [==============================] - 91s 463ms/step - loss: 0.1368 - val_loss: 0.2281\n","Epoch 74/300\n","196/196 [==============================] - 92s 467ms/step - loss: 0.1353 - val_loss: 0.2288\n","Epoch 75/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.1340 - val_loss: 0.2281\n","Epoch 76/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.1329 - val_loss: 0.2280\n","Epoch 77/300\n","196/196 [==============================] - 95s 484ms/step - loss: 0.1320 - val_loss: 0.2275\n","Epoch 78/300\n","196/196 [==============================] - 93s 476ms/step - loss: 0.1312 - val_loss: 0.2278\n","Epoch 79/300\n","196/196 [==============================] - 94s 481ms/step - loss: 0.1298 - val_loss: 0.2271\n","Epoch 80/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.1289 - val_loss: 0.2276\n","Epoch 81/300\n","196/196 [==============================] - 99s 504ms/step - loss: 0.1277 - val_loss: 0.2277\n","Epoch 82/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.1269 - val_loss: 0.2281\n","Epoch 83/300\n","196/196 [==============================] - 101s 514ms/step - loss: 0.1256 - val_loss: 0.2278\n","Epoch 84/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.1243 - val_loss: 0.2278\n","Epoch 85/300\n","196/196 [==============================] - 105s 535ms/step - loss: 0.1234 - val_loss: 0.2277\n","Epoch 86/300\n","196/196 [==============================] - 101s 518ms/step - loss: 0.1222 - val_loss: 0.2276\n","Epoch 87/300\n","196/196 [==============================] - 100s 509ms/step - loss: 0.1216 - val_loss: 0.2278\n","Epoch 88/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.1212 - val_loss: 0.2281\n","Epoch 89/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.1200 - val_loss: 0.2281\n","Epoch 90/300\n","196/196 [==============================] - 102s 522ms/step - loss: 0.1189 - val_loss: 0.2286\n","Epoch 91/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.1179 - val_loss: 0.2273\n","Epoch 92/300\n","196/196 [==============================] - 102s 520ms/step - loss: 0.1162 - val_loss: 0.2281\n","Epoch 93/300\n","196/196 [==============================] - 101s 517ms/step - loss: 0.1155 - val_loss: 0.2288\n","Epoch 94/300\n","196/196 [==============================] - 101s 515ms/step - loss: 0.1149 - val_loss: 0.2281\n","Epoch 95/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.1137 - val_loss: 0.2278\n","Epoch 96/300\n","196/196 [==============================] - 100s 508ms/step - loss: 0.1128 - val_loss: 0.2286\n","Epoch 97/300\n","196/196 [==============================] - 98s 501ms/step - loss: 0.1114 - val_loss: 0.2278\n","Epoch 98/300\n","196/196 [==============================] - 97s 497ms/step - loss: 0.1109 - val_loss: 0.2288\n","Epoch 99/300\n","196/196 [==============================] - 98s 501ms/step - loss: 0.1106 - val_loss: 0.2278\n","Epoch 100/300\n","196/196 [==============================] - 99s 506ms/step - loss: 0.1091 - val_loss: 0.2286\n","Epoch 101/300\n","196/196 [==============================] - 98s 498ms/step - loss: 0.1083 - val_loss: 0.2292\n","Epoch 102/300\n","196/196 [==============================] - 98s 502ms/step - loss: 0.1078 - val_loss: 0.2283\n","Epoch 103/300\n","196/196 [==============================] - 99s 504ms/step - loss: 0.1066 - val_loss: 0.2297\n","Epoch 104/300\n","196/196 [==============================] - 95s 484ms/step - loss: 0.1057 - val_loss: 0.2295\n","Epoch 105/300\n","196/196 [==============================] - 97s 497ms/step - loss: 0.1043 - val_loss: 0.2298\n","Epoch 106/300\n","196/196 [==============================] - 99s 504ms/step - loss: 0.1036 - val_loss: 0.2292\n","Epoch 107/300\n","196/196 [==============================] - 100s 511ms/step - loss: 0.1029 - val_loss: 0.2291\n","Epoch 108/300\n","196/196 [==============================] - 98s 499ms/step - loss: 0.1024 - val_loss: 0.2299\n","Epoch 109/300\n","196/196 [==============================] - 96s 488ms/step - loss: 0.1015 - val_loss: 0.2294\n","Epoch 110/300\n","196/196 [==============================] - 95s 485ms/step - loss: 0.1002 - val_loss: 0.2300\n","Epoch 111/300\n","196/196 [==============================] - 97s 494ms/step - loss: 0.0996 - val_loss: 0.2305\n","Epoch 112/300\n","196/196 [==============================] - 97s 497ms/step - loss: 0.0989 - val_loss: 0.2306\n","Epoch 113/300\n","196/196 [==============================] - 98s 501ms/step - loss: 0.0976 - val_loss: 0.2301\n","Epoch 114/300\n","196/196 [==============================] - 100s 509ms/step - loss: 0.0972 - val_loss: 0.2302\n","Epoch 115/300\n","196/196 [==============================] - 102s 518ms/step - loss: 0.0970 - val_loss: 0.2308\n","Epoch 116/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.0957 - val_loss: 0.2315\n","Epoch 117/300\n","196/196 [==============================] - 97s 493ms/step - loss: 0.0945 - val_loss: 0.2312\n","Epoch 118/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.0946 - val_loss: 0.2319\n","Epoch 119/300\n","196/196 [==============================] - 100s 508ms/step - loss: 0.0930 - val_loss: 0.2311\n","Epoch 120/300\n","196/196 [==============================] - 98s 499ms/step - loss: 0.0928 - val_loss: 0.2321\n","Epoch 121/300\n","196/196 [==============================] - 98s 503ms/step - loss: 0.0917 - val_loss: 0.2317\n","Epoch 122/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0908 - val_loss: 0.2326\n","Epoch 123/300\n","196/196 [==============================] - 101s 514ms/step - loss: 0.0901 - val_loss: 0.2318\n","Epoch 124/300\n","196/196 [==============================] - 101s 513ms/step - loss: 0.0896 - val_loss: 0.2327\n","Epoch 125/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0888 - val_loss: 0.2325\n","Epoch 126/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0877 - val_loss: 0.2330\n","Epoch 127/300\n","196/196 [==============================] - 110s 564ms/step - loss: 0.0875 - val_loss: 0.2334\n","Epoch 128/300\n","196/196 [==============================] - 107s 546ms/step - loss: 0.0868 - val_loss: 0.2343\n","Epoch 129/300\n","196/196 [==============================] - 98s 498ms/step - loss: 0.0856 - val_loss: 0.2343\n","Epoch 130/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.0853 - val_loss: 0.2343\n","Epoch 131/300\n","196/196 [==============================] - 90s 458ms/step - loss: 0.0846 - val_loss: 0.2339\n","Epoch 132/300\n","196/196 [==============================] - 90s 460ms/step - loss: 0.0839 - val_loss: 0.2349\n","Epoch 133/300\n","196/196 [==============================] - 90s 461ms/step - loss: 0.0834 - val_loss: 0.2348\n","Epoch 134/300\n","196/196 [==============================] - 90s 459ms/step - loss: 0.0827 - val_loss: 0.2345\n","Epoch 135/300\n","196/196 [==============================] - 90s 458ms/step - loss: 0.0816 - val_loss: 0.2350\n","Epoch 136/300\n","196/196 [==============================] - 91s 463ms/step - loss: 0.0809 - val_loss: 0.2354\n","Epoch 137/300\n","196/196 [==============================] - 91s 466ms/step - loss: 0.0801 - val_loss: 0.2356\n","Epoch 138/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.0796 - val_loss: 0.2362\n","Epoch 139/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.0792 - val_loss: 0.2356\n","Epoch 140/300\n","196/196 [==============================] - 93s 476ms/step - loss: 0.0782 - val_loss: 0.2358\n","Epoch 141/300\n","196/196 [==============================] - 92s 471ms/step - loss: 0.0778 - val_loss: 0.2368\n","Epoch 142/300\n","196/196 [==============================] - 92s 472ms/step - loss: 0.0766 - val_loss: 0.2368\n","Epoch 143/300\n","196/196 [==============================] - 94s 481ms/step - loss: 0.0761 - val_loss: 0.2376\n","Epoch 144/300\n","196/196 [==============================] - 95s 486ms/step - loss: 0.0764 - val_loss: 0.2376\n","Epoch 145/300\n","196/196 [==============================] - 96s 488ms/step - loss: 0.0750 - val_loss: 0.2376\n","Epoch 146/300\n","196/196 [==============================] - 95s 484ms/step - loss: 0.0741 - val_loss: 0.2382\n","Epoch 147/300\n","196/196 [==============================] - 95s 484ms/step - loss: 0.0735 - val_loss: 0.2382\n","Epoch 148/300\n","196/196 [==============================] - 92s 472ms/step - loss: 0.0731 - val_loss: 0.2388\n","Epoch 149/300\n","196/196 [==============================] - 93s 475ms/step - loss: 0.0722 - val_loss: 0.2387\n","Epoch 150/300\n","196/196 [==============================] - 90s 461ms/step - loss: 0.0718 - val_loss: 0.2389\n","Epoch 151/300\n","196/196 [==============================] - 91s 463ms/step - loss: 0.0713 - val_loss: 0.2393\n","Epoch 152/300\n","196/196 [==============================] - 93s 472ms/step - loss: 0.0706 - val_loss: 0.2397\n","Epoch 153/300\n","196/196 [==============================] - 92s 470ms/step - loss: 0.0700 - val_loss: 0.2395\n","Epoch 154/300\n","196/196 [==============================] - 91s 464ms/step - loss: 0.0695 - val_loss: 0.2399\n","Epoch 155/300\n","196/196 [==============================] - 92s 469ms/step - loss: 0.0690 - val_loss: 0.2397\n","Epoch 156/300\n","196/196 [==============================] - 92s 468ms/step - loss: 0.0688 - val_loss: 0.2406\n","Epoch 157/300\n","196/196 [==============================] - 92s 471ms/step - loss: 0.0677 - val_loss: 0.2406\n","Epoch 158/300\n","196/196 [==============================] - 92s 470ms/step - loss: 0.0669 - val_loss: 0.2419\n","Epoch 159/300\n","196/196 [==============================] - 98s 502ms/step - loss: 0.0664 - val_loss: 0.2417\n","Epoch 160/300\n","196/196 [==============================] - 99s 505ms/step - loss: 0.0655 - val_loss: 0.2420\n","Epoch 161/300\n","196/196 [==============================] - 98s 500ms/step - loss: 0.0652 - val_loss: 0.2412\n","Epoch 162/300\n","196/196 [==============================] - 102s 521ms/step - loss: 0.0645 - val_loss: 0.2428\n","Epoch 163/300\n","196/196 [==============================] - 108s 550ms/step - loss: 0.0640 - val_loss: 0.2425\n","Epoch 164/300\n","196/196 [==============================] - 105s 537ms/step - loss: 0.0635 - val_loss: 0.2429\n","Epoch 165/300\n","196/196 [==============================] - 106s 539ms/step - loss: 0.0632 - val_loss: 0.2433\n","Epoch 166/300\n","196/196 [==============================] - 107s 545ms/step - loss: 0.0627 - val_loss: 0.2433\n","Epoch 167/300\n","196/196 [==============================] - 106s 541ms/step - loss: 0.0620 - val_loss: 0.2437\n","Epoch 168/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0612 - val_loss: 0.2443\n","Epoch 169/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.0609 - val_loss: 0.2450\n","Epoch 170/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0597 - val_loss: 0.2446\n","Epoch 171/300\n","196/196 [==============================] - 105s 536ms/step - loss: 0.0599 - val_loss: 0.2451\n","Epoch 172/300\n","196/196 [==============================] - 106s 543ms/step - loss: 0.0592 - val_loss: 0.2447\n","Epoch 173/300\n","196/196 [==============================] - 107s 548ms/step - loss: 0.0586 - val_loss: 0.2459\n","Epoch 174/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0579 - val_loss: 0.2465\n","Epoch 175/300\n","196/196 [==============================] - 107s 546ms/step - loss: 0.0575 - val_loss: 0.2467\n","Epoch 176/300\n","196/196 [==============================] - 107s 547ms/step - loss: 0.0571 - val_loss: 0.2456\n","Epoch 177/300\n","196/196 [==============================] - 107s 547ms/step - loss: 0.0567 - val_loss: 0.2475\n","Epoch 178/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0562 - val_loss: 0.2474\n","Epoch 179/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0554 - val_loss: 0.2469\n","Epoch 180/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0550 - val_loss: 0.2475\n","Epoch 181/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0541 - val_loss: 0.2477\n","Epoch 182/300\n","196/196 [==============================] - 100s 509ms/step - loss: 0.0537 - val_loss: 0.2475\n","Epoch 183/300\n","196/196 [==============================] - 103s 527ms/step - loss: 0.0535 - val_loss: 0.2485\n","Epoch 184/300\n","196/196 [==============================] - 103s 523ms/step - loss: 0.0527 - val_loss: 0.2496\n","Epoch 185/300\n","196/196 [==============================] - 103s 524ms/step - loss: 0.0525 - val_loss: 0.2490\n","Epoch 186/300\n","196/196 [==============================] - 105s 537ms/step - loss: 0.0518 - val_loss: 0.2503\n","Epoch 187/300\n","196/196 [==============================] - 104s 531ms/step - loss: 0.0512 - val_loss: 0.2507\n","Epoch 188/300\n","196/196 [==============================] - 106s 539ms/step - loss: 0.0510 - val_loss: 0.2503\n","Epoch 189/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0507 - val_loss: 0.2502\n","Epoch 190/300\n","196/196 [==============================] - 104s 533ms/step - loss: 0.0507 - val_loss: 0.2511\n","Epoch 191/300\n","196/196 [==============================] - 103s 528ms/step - loss: 0.0498 - val_loss: 0.2513\n","Epoch 192/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0493 - val_loss: 0.2523\n","Epoch 193/300\n","196/196 [==============================] - 107s 548ms/step - loss: 0.0490 - val_loss: 0.2515\n","Epoch 194/300\n","196/196 [==============================] - 106s 541ms/step - loss: 0.0479 - val_loss: 0.2524\n","Epoch 195/300\n","196/196 [==============================] - 108s 552ms/step - loss: 0.0477 - val_loss: 0.2529\n","Epoch 196/300\n","196/196 [==============================] - 108s 551ms/step - loss: 0.0475 - val_loss: 0.2523\n","Epoch 197/300\n","196/196 [==============================] - 109s 555ms/step - loss: 0.0467 - val_loss: 0.2535\n","Epoch 198/300\n","196/196 [==============================] - 107s 546ms/step - loss: 0.0461 - val_loss: 0.2536\n","Epoch 199/300\n","196/196 [==============================] - 105s 536ms/step - loss: 0.0453 - val_loss: 0.2547\n","Epoch 200/300\n","196/196 [==============================] - 106s 543ms/step - loss: 0.0454 - val_loss: 0.2546\n","Epoch 201/300\n","196/196 [==============================] - 105s 537ms/step - loss: 0.0452 - val_loss: 0.2548\n","Epoch 202/300\n","196/196 [==============================] - 105s 535ms/step - loss: 0.0444 - val_loss: 0.2555\n","Epoch 203/300\n","196/196 [==============================] - 104s 533ms/step - loss: 0.0441 - val_loss: 0.2558\n","Epoch 204/300\n","196/196 [==============================] - 106s 543ms/step - loss: 0.0438 - val_loss: 0.2553\n","Epoch 205/300\n","196/196 [==============================] - 107s 547ms/step - loss: 0.0432 - val_loss: 0.2559\n","Epoch 206/300\n","196/196 [==============================] - 109s 555ms/step - loss: 0.0425 - val_loss: 0.2564\n","Epoch 207/300\n","196/196 [==============================] - 107s 544ms/step - loss: 0.0422 - val_loss: 0.2565\n","Epoch 208/300\n","196/196 [==============================] - 110s 560ms/step - loss: 0.0416 - val_loss: 0.2569\n","Epoch 209/300\n","196/196 [==============================] - 109s 557ms/step - loss: 0.0415 - val_loss: 0.2569\n","Epoch 210/300\n","196/196 [==============================] - 109s 556ms/step - loss: 0.0408 - val_loss: 0.2577\n","Epoch 211/300\n","196/196 [==============================] - 110s 561ms/step - loss: 0.0403 - val_loss: 0.2591\n","Epoch 212/300\n","196/196 [==============================] - 108s 553ms/step - loss: 0.0401 - val_loss: 0.2584\n","Epoch 213/300\n","196/196 [==============================] - 111s 564ms/step - loss: 0.0400 - val_loss: 0.2607\n","Epoch 214/300\n","196/196 [==============================] - 105s 535ms/step - loss: 0.0396 - val_loss: 0.2606\n","Epoch 215/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0397 - val_loss: 0.2611\n","Epoch 216/300\n","196/196 [==============================] - 103s 527ms/step - loss: 0.0389 - val_loss: 0.2602\n","Epoch 217/300\n","196/196 [==============================] - 104s 531ms/step - loss: 0.0391 - val_loss: 0.2612\n","Epoch 218/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0384 - val_loss: 0.2616\n","Epoch 219/300\n","196/196 [==============================] - 103s 523ms/step - loss: 0.0376 - val_loss: 0.2611\n","Epoch 220/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.0373 - val_loss: 0.2626\n","Epoch 221/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0369 - val_loss: 0.2626\n","Epoch 222/300\n","196/196 [==============================] - 108s 549ms/step - loss: 0.0365 - val_loss: 0.2622\n","Epoch 223/300\n","196/196 [==============================] - 105s 533ms/step - loss: 0.0358 - val_loss: 0.2633\n","Epoch 224/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.0354 - val_loss: 0.2631\n","Epoch 225/300\n","196/196 [==============================] - 103s 526ms/step - loss: 0.0351 - val_loss: 0.2649\n","Epoch 226/300\n","196/196 [==============================] - 105s 534ms/step - loss: 0.0348 - val_loss: 0.2641\n","Epoch 227/300\n","196/196 [==============================] - 102s 522ms/step - loss: 0.0342 - val_loss: 0.2646\n","Epoch 228/300\n","196/196 [==============================] - 97s 493ms/step - loss: 0.0339 - val_loss: 0.2650\n","Epoch 229/300\n","196/196 [==============================] - 97s 495ms/step - loss: 0.0341 - val_loss: 0.2654\n","Epoch 230/300\n","196/196 [==============================] - 96s 489ms/step - loss: 0.0335 - val_loss: 0.2661\n","Epoch 231/300\n","196/196 [==============================] - 98s 498ms/step - loss: 0.0328 - val_loss: 0.2661\n","Epoch 232/300\n","196/196 [==============================] - 102s 518ms/step - loss: 0.0326 - val_loss: 0.2669\n","Epoch 233/300\n","196/196 [==============================] - 101s 514ms/step - loss: 0.0322 - val_loss: 0.2679\n","Epoch 234/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0320 - val_loss: 0.2686\n","Epoch 235/300\n","196/196 [==============================] - 106s 539ms/step - loss: 0.0315 - val_loss: 0.2682\n","Epoch 236/300\n","196/196 [==============================] - 104s 531ms/step - loss: 0.0309 - val_loss: 0.2691\n","Epoch 237/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0309 - val_loss: 0.2687\n","Epoch 238/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0307 - val_loss: 0.2692\n","Epoch 239/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0302 - val_loss: 0.2701\n","Epoch 240/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.0298 - val_loss: 0.2700\n","Epoch 241/300\n","196/196 [==============================] - 105s 536ms/step - loss: 0.0296 - val_loss: 0.2708\n","Epoch 242/300\n","196/196 [==============================] - 101s 515ms/step - loss: 0.0294 - val_loss: 0.2708\n","Epoch 243/300\n","196/196 [==============================] - 105s 537ms/step - loss: 0.0291 - val_loss: 0.2713\n","Epoch 244/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0287 - val_loss: 0.2718\n","Epoch 245/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.0284 - val_loss: 0.2718\n","Epoch 246/300\n","196/196 [==============================] - 103s 524ms/step - loss: 0.0283 - val_loss: 0.2726\n","Epoch 247/300\n","196/196 [==============================] - 102s 523ms/step - loss: 0.0281 - val_loss: 0.2723\n","Epoch 248/300\n","196/196 [==============================] - 100s 508ms/step - loss: 0.0280 - val_loss: 0.2733\n","Epoch 249/300\n","196/196 [==============================] - 101s 514ms/step - loss: 0.0277 - val_loss: 0.2736\n","Epoch 250/300\n","196/196 [==============================] - 101s 517ms/step - loss: 0.0274 - val_loss: 0.2736\n","Epoch 251/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.0271 - val_loss: 0.2739\n","Epoch 252/300\n","196/196 [==============================] - 104s 531ms/step - loss: 0.0267 - val_loss: 0.2742\n","Epoch 253/300\n","196/196 [==============================] - 103s 527ms/step - loss: 0.0262 - val_loss: 0.2752\n","Epoch 254/300\n","196/196 [==============================] - 105s 533ms/step - loss: 0.0261 - val_loss: 0.2751\n","Epoch 255/300\n","196/196 [==============================] - 106s 542ms/step - loss: 0.0256 - val_loss: 0.2753\n","Epoch 256/300\n","196/196 [==============================] - 107s 544ms/step - loss: 0.0254 - val_loss: 0.2763\n","Epoch 257/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0247 - val_loss: 0.2760\n","Epoch 258/300\n","196/196 [==============================] - 105s 536ms/step - loss: 0.0243 - val_loss: 0.2762\n","Epoch 259/300\n","196/196 [==============================] - 105s 538ms/step - loss: 0.0242 - val_loss: 0.2768\n","Epoch 260/300\n","196/196 [==============================] - 106s 542ms/step - loss: 0.0239 - val_loss: 0.2769\n","Epoch 261/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0234 - val_loss: 0.2780\n","Epoch 262/300\n","196/196 [==============================] - 107s 546ms/step - loss: 0.0233 - val_loss: 0.2787\n","Epoch 263/300\n","196/196 [==============================] - 107s 548ms/step - loss: 0.0229 - val_loss: 0.2788\n","Epoch 264/300\n","196/196 [==============================] - 108s 551ms/step - loss: 0.0227 - val_loss: 0.2790\n","Epoch 265/300\n","196/196 [==============================] - 104s 532ms/step - loss: 0.0225 - val_loss: 0.2792\n","Epoch 266/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0223 - val_loss: 0.2795\n","Epoch 267/300\n","196/196 [==============================] - 108s 549ms/step - loss: 0.0223 - val_loss: 0.2799\n","Epoch 268/300\n","196/196 [==============================] - 107s 548ms/step - loss: 0.0227 - val_loss: 0.2811\n","Epoch 269/300\n","196/196 [==============================] - 107s 544ms/step - loss: 0.0221 - val_loss: 0.2810\n","Epoch 270/300\n","196/196 [==============================] - 110s 561ms/step - loss: 0.0217 - val_loss: 0.2808\n","Epoch 271/300\n","196/196 [==============================] - 105s 534ms/step - loss: 0.0213 - val_loss: 0.2810\n","Epoch 272/300\n","196/196 [==============================] - 109s 555ms/step - loss: 0.0211 - val_loss: 0.2821\n","Epoch 273/300\n","196/196 [==============================] - 111s 567ms/step - loss: 0.0208 - val_loss: 0.2824\n","Epoch 274/300\n","196/196 [==============================] - 109s 556ms/step - loss: 0.0204 - val_loss: 0.2836\n","Epoch 275/300\n","196/196 [==============================] - 108s 552ms/step - loss: 0.0202 - val_loss: 0.2828\n","Epoch 276/300\n","196/196 [==============================] - 110s 561ms/step - loss: 0.0200 - val_loss: 0.2839\n","Epoch 277/300\n","196/196 [==============================] - 108s 552ms/step - loss: 0.0198 - val_loss: 0.2846\n","Epoch 278/300\n","196/196 [==============================] - 105s 538ms/step - loss: 0.0197 - val_loss: 0.2842\n","Epoch 279/300\n","196/196 [==============================] - 107s 548ms/step - loss: 0.0196 - val_loss: 0.2852\n","Epoch 280/300\n","196/196 [==============================] - 105s 536ms/step - loss: 0.0198 - val_loss: 0.2855\n","Epoch 281/300\n","196/196 [==============================] - 104s 528ms/step - loss: 0.0192 - val_loss: 0.2853\n","Epoch 282/300\n","196/196 [==============================] - 105s 537ms/step - loss: 0.0190 - val_loss: 0.2854\n","Epoch 283/300\n","196/196 [==============================] - 106s 540ms/step - loss: 0.0188 - val_loss: 0.2863\n","Epoch 284/300\n","196/196 [==============================] - 100s 512ms/step - loss: 0.0186 - val_loss: 0.2865\n","Epoch 285/300\n","196/196 [==============================] - 99s 506ms/step - loss: 0.0185 - val_loss: 0.2871\n","Epoch 286/300\n","196/196 [==============================] - 97s 496ms/step - loss: 0.0183 - val_loss: 0.2871\n","Epoch 287/300\n","196/196 [==============================] - 97s 495ms/step - loss: 0.0184 - val_loss: 0.2873\n","Epoch 288/300\n","196/196 [==============================] - 98s 501ms/step - loss: 0.0180 - val_loss: 0.2873\n","Epoch 289/300\n","196/196 [==============================] - 102s 519ms/step - loss: 0.0178 - val_loss: 0.2885\n","Epoch 290/300\n","196/196 [==============================] - 104s 529ms/step - loss: 0.0172 - val_loss: 0.2883\n","Epoch 291/300\n","196/196 [==============================] - 104s 530ms/step - loss: 0.0170 - val_loss: 0.2874\n","Epoch 292/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.0174 - val_loss: 0.2877\n","Epoch 293/300\n","196/196 [==============================] - 103s 525ms/step - loss: 0.0170 - val_loss: 0.2890\n","Epoch 294/300\n","196/196 [==============================] - 104s 528ms/step - loss: 0.0166 - val_loss: 0.2895\n","Epoch 295/300\n","196/196 [==============================] - 102s 521ms/step - loss: 0.0166 - val_loss: 0.2892\n","Epoch 296/300\n","196/196 [==============================] - 100s 511ms/step - loss: 0.0163 - val_loss: 0.2907\n","Epoch 297/300\n","196/196 [==============================] - 104s 528ms/step - loss: 0.0160 - val_loss: 0.2912\n","Epoch 298/300\n","196/196 [==============================] - 103s 524ms/step - loss: 0.0159 - val_loss: 0.2910\n","Epoch 299/300\n","196/196 [==============================] - 103s 526ms/step - loss: 0.0157 - val_loss: 0.2913\n","Epoch 300/300\n","196/196 [==============================] - 103s 526ms/step - loss: 0.0156 - val_loss: 0.2909\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eZv-450GDU2q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S72_XcbJ2P6","colab_type":"code","colab":{}},"source":["### ### ### ### ### ### ### #\n","# SAVING THE TRAINED MODELS #\n","### ### ### ### ### ### ### #\n","\n","encoder_model.save(home_dir + \"Models/\" + config_number_str + '_encoder_model.h5')  \n","decoder_model.save(home_dir + \"Models/\" + config_number_str + '_decoder_model.h5')  "],"execution_count":0,"outputs":[]}]}